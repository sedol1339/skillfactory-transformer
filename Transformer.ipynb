{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Transformer задание",
      "provenance": [],
      "collapsed_sections": [
        "CuqzsOEIVVQb",
        "1yoDwZtCWBmF",
        "bZ8xq_ZdWnpq",
        "EzCByHgTYwYd",
        "jGoO8docYFgV",
        "qwOlLPZUYLgq",
        "yQnnNFIsYj8t",
        "W61nf8SDYq-w",
        "A4jQ8sb9Z7lz",
        "3su6k_uhaS0n",
        "4M4SLd1caW1u",
        "Z6oH7SveadDN",
        "ZmZwiSH8cCIh",
        "8AWfB1nSdIPS",
        "O_VJUuqgd3uq",
        "DngaA09UeKMp",
        "r-h84S67exmO",
        "j_8EABkwfcyo"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbD5w8UdENa1"
      },
      "source": [
        "# Введение\n",
        "\n",
        "<style>\n",
        "a:link {\n",
        "  color: green;\n",
        "  background-color: transparent;\n",
        "  text-decoration: none;\n",
        "}\n",
        "</style>\n",
        "\n",
        "Данный ноутбук закрепляет на практике понимание архитектуры \"трансформер\", которая была предложена группой исследователей из Google в 2017 году в статье [Attention Is All You Need](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "Трансформер и его модификации получили широкое распространение в задачах NLP ([BERT](https://arxiv.org/abs/1810.04805), [ALBERT](https://arxiv.org/abs/1909.11942v6), [RoBERTa](https://arxiv.org/abs/1907.11692), [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [GPT-3](https://arxiv.org/abs/2005.14165), [ruGPT-3](https://sbercloud.ru/ru/warp/gpt-3), [Turing-NLG](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/), [Switch Transformer](https://arxiv.org/abs/2101.03961), [EFL](https://arxiv.org/abs/2104.14690v1), [Reformer](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html), [T5](https://arxiv.org/abs/1910.10683) и много других). Языковые модели, основанные на трансформерах, способны генерировать [тексты](https://mobile.twitter.com/raphamilliere/status/1289129723310886912), почти неотличимые от написанных человеком. Трансформеры также начинают активно использоваться в компьютерном зрении ([ViT](https://arxiv.org/abs/2106.04803v2), [HRNet](https://arxiv.org/abs/1909.11065v6), [SwinIR](https://arxiv.org/abs/2108.10257) и др., см. также [здесь](https://habr.com/ru/post/578308/) и [здесь](https://arxiv.org/abs/2101.01169)), в мультимодальных сетях, связанных одновременно с изображениями и текстом ([CLIP](https://openai.com/blog/clip/), [DALL-E](https://openai.com/blog/dall-e/), [Wu Dao](https://www.forbes.com/sites/alexzhavoronkov/2021/07/19/wu-dao-20bigger-stronger-faster-ai-from-china/) и др.), и даже в предсказании структуры белков ([AlphaFold2](https://www.nature.com/articles/s41586-021-03819-2)). Более того, есть [работа](https://arxiv.org/abs/2103.05247), показывающая, что трансформер может претендовать на роль универсальной архитектуры для одновременного решения самых разных задач.\n",
        "\n",
        "Трансформер - непростая архитектура, и перед тем, как переходить к практике построения трансформеров, рекомендуется хорошо изучить принцип их устройства. Для этого вы можете использовать следующие материалы:\n",
        "\n",
        "1. Материалы обучающего модуля курса NLP-инженер\n",
        "1. [Детальное описание устройства трансформера](https://limitless-depths-73156.herokuapp.com/Attention_Is_All_You_Need) от автора данного ноутбука\n",
        "1. Научную статью [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (англ.)\n",
        "1. Статью [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) (англ.) и [перевод на Хабре](https://habr.com/ru/post/486358/)\n",
        "1. [Часть обучающего курса про трансформеры от Лены Войты](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html) (англ.)\n",
        "1. [Практику по трансформерам от UvA Deep Learning Tutorials](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html) (англ.)\n",
        "1. Экспертов поддержки в Slack, которым без вас скучно :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrsVgd5qEtcm"
      },
      "source": [
        "# Часть 1\n",
        "\n",
        "В этой части мы рассмотрим практическую реализацию механизма self-attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfFiCHbrUT6w"
      },
      "source": [
        "### Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltIkYUpzUYlb"
      },
      "source": [
        "Дан вариант реализации scaled dot-product self-attention с помощью матричных операций:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aMLwMQlDe-_"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, masked=False):\n",
        "    '''Scaled dot product attention from https://arxiv.org/abs/1706.03762\n",
        "    Args:\n",
        "        Q - query, torch.Tensor of shape (batch_size, num_queries, key_size)\n",
        "        K - keys, torch.Tensor of shape (batch_size, num_values, key_size)\n",
        "        V - values, torch.Tensor of shape (batch_size, num_values, value_size)\n",
        "        masked - [not implemented] boolean: should we use attention mask?\n",
        "    Returns:\n",
        "        torch.Tensor of shape (batch_size, num_queries, value_size)\n",
        "    '''\n",
        "    assert Q.ndim == K.ndim == V.ndim == 3\n",
        "    assert Q.shape[0] == K.shape[0] == V.shape[0] # batch_size\n",
        "    assert Q.shape[2] == K.shape[2] # key_size\n",
        "    assert K.shape[1] == V.shape[1] # num_values\n",
        "    scalar_products = Q @ K.transpose(1, 2)\n",
        "    scalar_products /= K.shape[2]\n",
        "    weights = F.softmax(scalar_products, dim=1)\n",
        "    return weights @ V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHzAirjfRN48"
      },
      "source": [
        "Запустим эту функцию, передав ей массив из единиц:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrOwFDo4T5-0",
        "outputId": "66af5479-e5b8-4f88-9d7c-951b4302f291"
      },
      "source": [
        "Q = torch.ones((2, 4, 6))\n",
        "K = torch.ones((2, 8, 6))\n",
        "V = torch.ones((2, 8, 10))\n",
        "scaled_dot_product_attention(Q, K, V)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]],\n",
              "\n",
              "        [[2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
              "         [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eukxFankUQbz"
      },
      "source": [
        "Получился массив из двоек. Но мы считаем взвешенное среднее массива из единиц, как же мы получаем двойки? Так и должно быть, или где-то в коде функции `scaled_dot_product_attention` ошибка? А может быть мы неправильно используем эту функцию?\n",
        "\n",
        "Если вы считаете, что в коде ошибка, то исправьте ее."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuqzsOEIVVQb"
      },
      "source": [
        "### Решение задания 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-cItm9MVZZy"
      },
      "source": [
        "Давайте посмотрим на веса. Для этого в функцию scaled_dot_product_attention добавим печать weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKFTeQCPVcdd"
      },
      "source": [
        "def scaled_dot_product_attention(Q, K, V, masked=False):\n",
        "    scalar_products = Q @ K.transpose(1, 2)\n",
        "    scalar_products /= K.shape[2]\n",
        "    weights = F.softmax(scalar_products, dim=1)\n",
        "    print(weights.shape, weights)\n",
        "    return weights @ V\n",
        "\n",
        "Q = torch.ones((2, 4, 6))\n",
        "K = torch.ones((2, 8, 6))\n",
        "V = torch.ones((2, 8, 10))\n",
        "print(scaled_dot_product_attention(Q, K, V))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yN7iZxBVeCH"
      },
      "source": [
        "Видим, что массив weights имеет размер (batch_size, num_queries, num_values). Здесь все правильно, так и должно быть. Массив weights заполнен значениями 0.25. Получается, что мы считаем взвешенное среднее 8 единичных векторов, используя веса 0.25. Сумма весов получается равной 2, но должна быть равна 1. Значит **в коде функции ошибка**.\n",
        "\n",
        "Корректно примененная операция softmax всегда должна выдавать значения, сумма которых равна единице. Значит **операция softmax применена некорректно**. Открываем [документацию](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) по softmax.\n",
        "\n",
        "> dim (int) – A dimension along which Softmax will be computed (so every slice along dim will sum to 1).\n",
        "\n",
        "Массив, который мы обрабатываем функцией softmax, имеет размер (2, 4, 8). Последняя ось отвечает за номер Value, поэтому softmax нужно применять по последней оси. В результате мы получим:\n",
        "\n",
        "`for every i, j: output[i, j, :].sum() == 1`,\n",
        "\n",
        "что нам и требуется. Значит в softmax надо использовать параметр dim=2 или dim=-1, что то же самое. Исправим код:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoMsS9GfUjhf"
      },
      "source": [
        "def scaled_dot_product_attention(Q, K, V, masked=False):\n",
        "    # Исправленный код\n",
        "    scalar_products = Q @ K.transpose(1, 2)\n",
        "    scalar_products /= K.shape[2]\n",
        "    weights = F.softmax(scalar_products, dim=2)\n",
        "    return weights @ V\n",
        "\n",
        "Q = torch.ones((2, 4, 6))\n",
        "K = torch.ones((2, 8, 6))\n",
        "V = torch.ones((2, 8, 10))\n",
        "print(scaled_dot_product_attention(Q, K, V))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rx2tdoIV9yt"
      },
      "source": [
        "Теперь функция выдает корректный результат."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yoDwZtCWBmF"
      },
      "source": [
        "### Задание 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4GJQ0mbRQMo"
      },
      "source": [
        "Добавьте возможность маскирования (параметр masked) в функции scaled_dot_product_attention.\n",
        "\n",
        "Зададим ограничение на входные параметры: при использовании маскирования num_queries должно быть равно num_values.\n",
        "\n",
        "*Подсказка:* смотрите [теоретический материал](https://limitless-depths-73156.herokuapp.com/Attention_Is_All_You_Need), раздел \"Masked attention\". Вам нужно реализовать то, что на иллюстрации изображено как *\"Masked (\"auto-regressive\") self-attention\"*.\n",
        "\n",
        "Для проверки используйте следующий код:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8O9zf0rGW8tl"
      },
      "source": [
        "# проверка маскирования в scaled_dot_product_attention\n",
        "\n",
        "def check(scaled_dot_product_attention):\n",
        "  _range = torch.arange(4, dtype=torch.float)\n",
        "  inputs = _range[None, :, None]\n",
        "  outputs = scaled_dot_product_attention(inputs, inputs, inputs, masked=True)\n",
        "  assert outputs.shape == (1, 4, 1)\n",
        "  for i in range(4):\n",
        "      assert outputs[0, i, 0] == (F.softmax(_range[i]*_range[:i+1], dim=0)*_range[:i+1]).sum()\n",
        "  print('Check passed!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ8xq_ZdWnpq"
      },
      "source": [
        "### Решение задания 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqfp0cniWpzS"
      },
      "source": [
        "`Scalar_products` в функции - это массив размером `(batch_size, num_queries, num_values)`. Для каждого примера в батче это матрица, где строка - номер `query`, столбец - номер `value`.\n",
        "\n",
        "Чтобы реализовать маскирование, нам нужно заменить некоторые элементы в `scalar_products` на минус бесконечность. Мы заменяем те элементы, где номер столбца больше номера строки, на минус бесконечность. Это можно сделать разными способами. Предпочтительно при этом использовать функции pytorch, а не numpy. Значит для того приходится обращаться к документации pytorch. Например, подойдет такой способ:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDoynsf1OFG1"
      },
      "source": [
        "def scaled_dot_product_attention(Q, K, V, masked=False):\n",
        "    scalar_products = Q @ K.transpose(1, 2)\n",
        "    scalar_products /= K.shape[2]\n",
        "\n",
        "    if masked:\n",
        "      i = torch.arange(scalar_products.shape[1])[:, None]\n",
        "      j = torch.arange(scalar_products.shape[2])[None, :]\n",
        "      mask = (i < j)\n",
        "      scalar_products.masked_fill_(mask, -np.inf)\n",
        "\n",
        "    weights = F.softmax(scalar_products, dim=2)\n",
        "    return weights @ V\n",
        "\n",
        "check(scaled_dot_product_attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N15jL8GXherz"
      },
      "source": [
        "# Часть 2\n",
        "\n",
        "В этой части мы будем строить блок энкодера. Изучите следующий код:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzCByHgTYwYd"
      },
      "source": [
        "### Код блока энкодера"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1ecUXBnh4Mj"
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, dropout=0.1, feedforward_dim=None,\n",
        "                 kdim=None, vdim=None, autoregressive=False):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            dim - integer: length of input and output vectors\n",
        "            num_heads - integer: Number of heads in multihead self-attention,\n",
        "                                 default: 8\n",
        "            dropout - float: dropout rate, default 0.1\n",
        "            feedforward_dim - integer: length of vectors between feedforward\n",
        "                                       layers, default: 4*dim\n",
        "            kdim - integer: size of keys, default: dim\n",
        "            vdim - integer: size of values, default: dim\n",
        "            autoregressive - boolean: should we use mask? default: False\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        feedforward_dim = feedforward_dim or 4*dim\n",
        "        kdim = kdim or dim\n",
        "        vdim = vdim or dim\n",
        "\n",
        "        self.self_attention = nn.MultiheadAttention(\n",
        "            embed_dim=dim,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True,\n",
        "            kdim=kdim,\n",
        "            vdim=vdim\n",
        "        )\n",
        "        \n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(dim, feedforward_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feedforward_dim, dim)\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim, eps=1e-5, elementwise_affine=True)\n",
        "        self.norm2 = nn.LayerNorm(dim, eps=1e-5, elementwise_affine=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.autoregressive = autoregressive\n",
        "\n",
        "    @staticmethod\n",
        "    def get_mask(len):\n",
        "      i = torch.arange(len)[:, None]\n",
        "      j = torch.arange(len)[None, :]\n",
        "      return i < j\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        attn_mask = self.get_mask(inputs.shape[1]) \\\n",
        "                     if self.autoregressive else None\n",
        "        residual, attn_wights = self.self_attention(inputs, inputs, inputs,\n",
        "                                                    attn_mask=attn_mask)\n",
        "        outputs = self.norm1(inputs + self.dropout(residual))\n",
        "\n",
        "        residual = self.feedforward(outputs)\n",
        "        outputs = self.norm2(outputs + self.dropout(residual))\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYnSqfQ6XeTG"
      },
      "source": [
        "Давайте проверим работу блока:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzez32OdXgfj"
      },
      "source": [
        "dim = 512\n",
        "seq_len = 20\n",
        "\n",
        "block = EncoderBlock(dim)\n",
        "inputs = torch.rand((1, seq_len, dim))\n",
        "outputs = block(inputs)\n",
        "assert outputs.shape == inputs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jok50lCqXiyo"
      },
      "source": [
        "Все работает!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGoO8docYFgV"
      },
      "source": [
        "### Задание 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe18uWwHY7wc"
      },
      "source": [
        "Проверим перестановочную эквивариантность блока. Это ключевое свойство трансформера: перестановка входных векторов приведет к тому, что аналогичным образом будут переставлены выходные вектора, но больше никак эти вектора не изменятся."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTIDwOpZY-KP"
      },
      "source": [
        "permutation = torch.randperm(seq_len)\n",
        "print(block(inputs[:, permutation, :]))\n",
        "print(block(inputs)[:, permutation, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMIXrG0_px9-"
      },
      "source": [
        "Как видим, перестановочная эквивариантность в блоке EncoderBlock не соблюдается: мы получаем разные вектора. Почему? Выясните причину, найдите ошибку и исправьте."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwOlLPZUYLgq"
      },
      "source": [
        "### Решение задания 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N58uPEmCYNtS"
      },
      "source": [
        "Блок состоит из 2 residual-блоков: self-attention и feedforward. Проведем эксперимент и увидим, что отключение любой из этих частей (комментирование соответствующего участка в методе `forward`) не исправляет ситуацию, то есть **обе части не являются перестановочно эквивариантными**. Неужели у нас двойная проблема? Давайте посмотрим сначала на ту часть, которая проще: на feedforward. Мы сразу видим в ней dropout. Оказалось, что все просто: нам нужно отключить dropout, переведя модель в режим инференса. В итоге мы получим одинаковые результаты:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgbDgaHco2Fi"
      },
      "source": [
        "block.eval()\n",
        "permutation = torch.randperm(seq_len)\n",
        "print(block(inputs[:, permutation, :]))\n",
        "print(block(inputs)[:, permutation, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQnnNFIsYj8t"
      },
      "source": [
        "### Задание 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2CaU9iBrWGu"
      },
      "source": [
        "В [одной из недавних работ](https://arxiv.org/abs/2002.04745) для более устойчивого обучения предлагается перенести\n",
        "`LayerNormalization` внутрь residual-блоков. Выполните эту модификацию, поставив `LayerNormalization` после `dropout`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W61nf8SDYq-w"
      },
      "source": [
        "### Решение задания 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iru4iL-OqZAT"
      },
      "source": [
        "def forward(self, inputs):\n",
        "    residual, attn_wights = self.self_attention(inputs, inputs, inputs)\n",
        "    outputs = inputs + self.norm1(self.dropout(residual))\n",
        "    outputs = inputs\n",
        "\n",
        "    residual = self.feedforward(outputs)\n",
        "    outputs = outputs + self.norm2(self.dropout(residual))\n",
        "\n",
        "    return outputs\n",
        "\n",
        "EncoderBlock.forward = forward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgPw_Ce72vPk"
      },
      "source": [
        "# Часть 3\n",
        "\n",
        "Эта часть - \"Proof of concept\". Мы проверим, обучается ли вообще нейросеть, которая построена с помощью нашего блока EncoderBlock. *Спойлер*: она обучается. Сложных заданий по поиску ошибки больше не будет."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4jQ8sb9Z7lz"
      },
      "source": [
        "### Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8MOji6TB1dH"
      },
      "source": [
        "!pip install pytorch-nlp -q\n",
        "!pip install pytorch_lightning -q\n",
        "!pip install torch-optimizer -q\n",
        "!wget -q https://storage.googleapis.com/oleg-zyablov/skillfactory/movie_classification/train.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am-AAwOHQoQm"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import math\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "import torch_optimizer as optim\n",
        "from torchnlp.word_to_vector import GloVe\n",
        "\n",
        "df = pd.read_csv('train.csv')\n",
        "texts = df.text.to_numpy() #исходные данные\n",
        "target = df.genre.to_numpy() #целевые данные"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3su6k_uhaS0n"
      },
      "source": [
        "### Задание 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ2-QgXBZ_y9"
      },
      "source": [
        "Проверьте точность случайного угадывания, разделим размер самого часто встречающегося класса на размер всего датасета:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M4SLd1caW1u"
      },
      "source": [
        "### Решение задания 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzPfC5hUQ9Oz"
      },
      "source": [
        "df.genre.value_counts()[0] / len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1agIZSvoaGau"
      },
      "source": [
        "Теперь мы знаем, какую точность должна превзойти модель, чтобы мы были уверены, что она хотя бы как-то обучается."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6oH7SveadDN"
      },
      "source": [
        "### Продолжаем готовить обучающие данные"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjgD_mlPaggF"
      },
      "source": [
        "- Выполним label-кодирование с помощью `sklearn.preprocessing.LabelEncoder`\n",
        "- Разобьем тексты на слова (токены)\n",
        "- Выполним label-кодирование слов с помощью `torchtext.vocab.Vocab`\n",
        "- Приведем каждый текст к длине в 256 слов добавлением токена `<unk>` или обрезкой\n",
        "- Создадим `torch.utils.data.TensorDataset`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N25jt3H6BC8N"
      },
      "source": [
        "target_encoder = LabelEncoder().fit(target)\n",
        "target = torch.tensor(target_encoder.transform(target), dtype=torch.long)\n",
        "num_classes = target.max() + 1\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "tokens = [tokenizer(x) for x in texts]\n",
        "\n",
        "vocab = build_vocab_from_iterator(tokens, specials=['<unk>'], min_freq=10)\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "token_indices = [torch.tensor(vocab(token_sequence), dtype=torch.long) for token_sequence in tokens]\n",
        "token_indices = torch.nn.utils.rnn.pad_sequence(token_indices, batch_first=True, padding_value=0)[:, :256]\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(token_indices[:40000], target[:40000])\n",
        "val_dataset = torch.utils.data.TensorDataset(token_indices[40000:], target[40000:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK7Jrmi6bi_M"
      },
      "source": [
        "- Загрузим какие-нибудь предобученные эмбеддинги слов, например GloVe (862 MB)\n",
        "- Создадим веса для слоя Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ2nkrqbMh7p"
      },
      "source": [
        "vectorizer = GloVe(name='6B', dim=300)\n",
        "embedding_weights = torch.stack([vectorizer[word] for word in vocab.get_itos()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmZwiSH8cCIh"
      },
      "source": [
        "### Создаем модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHWtOGI0b0cf"
      },
      "source": [
        "Будем использовать pytorch-lightning и сравнивать разные модели, начиная от самой простой, и заканчивая трансформером. Сразу переходить к трансформеру было бы неправильно. Вдруг окажется, что простейшая модель выдает точность не хуже трансформера? Иными словами, нам нужен некий бейзлайн.\n",
        "\n",
        "Вынесем все общие методы, не зависящие от архитектуры модели, в класс GeneralModel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhOpAhKVd45X"
      },
      "source": [
        "# general model for our task\n",
        "class GeneralModel(pl.LightningModule):\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        X, y = batch\n",
        "        preds = self.forward(X)\n",
        "        loss = F.cross_entropy(preds, y)\n",
        "        acc = (preds.argmax(dim=-1) == y).float().mean()\n",
        "        return {\"loss\": loss, \"acc\": acc}\n",
        "    def training_epoch_end(self, train_step_outputs):\n",
        "        avg_acc = torch.stack([x['acc'].float() for x in train_step_outputs]).mean()\n",
        "        print(f'train accuracy {avg_acc.cpu().numpy():g}')\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        X, y = batch\n",
        "        preds = self.forward(X)\n",
        "        acc = (preds.argmax(dim=-1) == y).float().mean()\n",
        "        #self.log('val_acc', acc)\n",
        "        return {'val_acc': acc}\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        avg_acc = torch.stack([x['val_acc'].float() for x in validation_step_outputs]).mean()\n",
        "        print(f'val accuracy {avg_acc.cpu().numpy():g}, ', end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzodSKHnceJT"
      },
      "source": [
        "Давайте создадим в качестве бейзлайна самую простую модель, какую можно придумать."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqHpHwv0cdga"
      },
      "source": [
        "# a simplest model\n",
        "class SimpleModel(GeneralModel):\n",
        "    def __init__(self, embedding_weights, num_classes):\n",
        "        super().__init__()\n",
        "        embedding_dim = embedding_weights.shape[1]\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=True)\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "    def forward(self, inputs):\n",
        "        raise NotImplementedError #TODO\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AWfB1nSdIPS"
      },
      "source": [
        "### Задание 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoHbBxpZdKvM"
      },
      "source": [
        "Напишите метод `forward` в модели `SimpleModel`. Метод должен выполнять 3 шага:\n",
        "1. Получаем эмбеддинги для входных слов с помощью слоя `self.embedding`. Для этого посмотрите, что модель принимает на вход (то есть на `train_dataset`, созданный ранее), и изучите принцип работы слоя `torch.nn.Embedding`.\n",
        "2. Усредняем все эмбеддинги. Тем самым получаем вектор фиксированной длины.\n",
        "3. Применяем классификатор `self.classifier`.\n",
        "\n",
        "Метод должен возвращать результат работы последнего слоя (классификатора).\n",
        "\n",
        "Чтобы проверить корректность вашего кода, запустите код ниже из раздела \"Обучаем простую модель (бейзлайн)\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_VJUuqgd3uq"
      },
      "source": [
        "### Решение задания 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTSUEUSDd61i"
      },
      "source": [
        "def forward(self, inputs):\n",
        "    embeddings = self.embedding(inputs)\n",
        "    mean = torch.mean(embeddings, dim=1)\n",
        "    logits = self.classifier(mean)\n",
        "    return logits\n",
        "\n",
        "SimpleModel.forward = forward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DngaA09UeKMp"
      },
      "source": [
        "### Обучаем простую модель (бейзлайн)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdxpBW7rdHSy"
      },
      "source": [
        "simple_model = SimpleModel(embedding_weights, num_classes)\n",
        "\n",
        "#overfit_batches=0.1, progress_bar_refresh_rate=0, log_every_n_steps=1\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
        "Trainer(gpus=1, num_sanity_val_steps=0, max_epochs=5, weights_summary=None).fit(simple_model, train_dataloader, val_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0scBykLetYM"
      },
      "source": [
        "Простая модель достигает точности на валидации выше 40%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-h84S67exmO"
      },
      "source": [
        "### Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JuTuEnhe08R"
      },
      "source": [
        "Для начала нам нужно запрограммировать positional encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OnCdtPgkN-D"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, dim, 2) * (-math.log(10000.0) / dim))\n",
        "        pe = torch.zeros(max_len, 1, dim)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK6WaYzDfBdR"
      },
      "source": [
        "Давайте проверим, что бейзлайн, дополненный positional encoding, не сильно теряет в точности. Конечно использовать positional encoding в модели, которая просто усредняет все эмбеддинги, бессмысленно. Но так мы по крайней мере проверим, что positional encoding ничего не \"ломает\" и не выдает ошибок. Можно считать это простым юнит-тестом для positional encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL9ZiNbGe9gS"
      },
      "source": [
        "class SimpleModelWithPE(GeneralModel):\n",
        "    def __init__(self, embedding_weights, num_classes):\n",
        "        super().__init__()\n",
        "        embedding_dim = embedding_weights.shape[1]\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=True)\n",
        "        self.pos_encoding = PositionalEncoding(embedding_dim, max_len=256, dropout=0.1)\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        embeddings = self.pos_encoding(embeddings)\n",
        "        mean = torch.mean(embeddings, dim=1)\n",
        "        logits = self.classifier(mean)\n",
        "        return logits\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "simple_model_pe = SimpleModelWithPE(embedding_weights, num_classes)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
        "Trainer(gpus=1, num_sanity_val_steps=0, max_epochs=5, weights_summary=None).fit(simple_model_pe, train_dataloader, val_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_8EABkwfcyo"
      },
      "source": [
        "### Создаем трансформер"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n15P6jHZferC"
      },
      "source": [
        "Финал!\n",
        "\n",
        "Для классификации не нужен декодер - достаточно только энкодера. Обычно для классификации текст дополняют специальным токеном `[CLS]`, и после последнего слоя трансформера считывают результат с этого токена. Затем передают полученный вектор в выходной слой-классификатор. Однако мы пойдем более простым путем, будем просто усреднять все выходные эмбеддинги (как и в простой модели). Усреднение будет выполняться после блоков трансформера.\n",
        "\n",
        "Для обучения будем использовать оптимизатор [`RAdam`](https://arxiv.org/abs/1908.03265), который отчасти похож на [`Adam`](https://arxiv.org/abs/1412.6980) с warmup.\n",
        "\n",
        "Ставим `max_epochs=500`. Обучение можно прервать в любой момент.\n",
        "\n",
        "Конечно трансформер будет обучаться намного дольше, но и достигнет более высокой точности, чем бейзлайн."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LRg3g2eVdhL"
      },
      "source": [
        "class TransformerPredictor(GeneralModel):\n",
        "    def __init__(self, embedding_weights, num_classes, num_heads=4, dropout=0.1, lr=1e-3):\n",
        "        super().__init__()\n",
        "        model_dim = embedding_weights.shape[1]\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=True)\n",
        "        self.pos_encoding = PositionalEncoding(model_dim, max_len=256, dropout=dropout)\n",
        "        self.cls_embedding = nn.Parameter(torch.zeros(model_dim))\n",
        "        # Можно использовать либо наш блок EncoderBlock:\n",
        "        self.encoder = nn.Sequential(*[\n",
        "            EncoderBlock(dim=model_dim, num_heads=num_heads, dropout=dropout)\n",
        "            for _ in range(6)\n",
        "        ])\n",
        "        # ... либо torch.nn.TransformerEncoder\n",
        "        # self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(\n",
        "        #     d_model=model_dim, nhead=num_heads), num_layers=3,\n",
        "        #     norm=nn.LayerNorm(normalized_shape=model_dim, eps=1e-6))\n",
        "        self.classifier = nn.Linear(model_dim, num_classes)\n",
        "    def forward(self, inputs):\n",
        "        embeddings = self.embedding(inputs)\n",
        "        embeddings = self.pos_encoding(embeddings)\n",
        "        encoder_outputs = self.encoder(embeddings)\n",
        "        encoder_outputs = torch.mean(embeddings, dim=1)\n",
        "        logits = self.classifier(encoder_outputs)\n",
        "        return logits\n",
        "    def configure_optimizers(self):\n",
        "        return optim.RAdam(self.parameters(), lr=1e-3)\n",
        "\n",
        "transformer = TransformerPredictor(embedding_weights, num_classes)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
        "Trainer(gpus=1, num_sanity_val_steps=0, max_epochs=500, log_every_n_steps=1,\n",
        "        weights_summary=None).fit(transformer, train_dataloader, val_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}